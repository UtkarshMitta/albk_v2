{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from os.path import expanduser, join, basename, dirname\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from shutil import copy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from albk.data.utils import idx_to_locate\n",
    "use_disjoint_files = False\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from glob import glob\n",
    "from os.path import expanduser, join, basename, dirname\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "from astra.torch.models import EfficientNetClassifier,EfficientNet_B0_Weights   \n",
    "from astra.torch.utils import train_fn\n",
    "\n",
    "import torchvision.models as models\n",
    "from astra.torch.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved tensors\n",
    "loaded_data = torch.load(\"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/tensor_data/data.pt\")\n",
    "\n",
    "# Access the tensors\n",
    "index = loaded_data['index']\n",
    "images = loaded_data['images']\n",
    "labels = loaded_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2000, 3, 224, 224]), torch.Size([2000]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images=images[:2000]\n",
    "labels=labels[:2000]\n",
    "images.shape, labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1369, 1: 131})\n",
      "Counter({0: 457, 1: 43})\n",
      "Fold 1 - Train: Counter({0: 1369, 1: 131}), Test: Counter({0: 457, 1: 43})\n",
      "Counter({0: 1369, 1: 131})\n",
      "Counter({0: 457, 1: 43})\n",
      "Fold 2 - Train: Counter({0: 1369, 1: 131}), Test: Counter({0: 457, 1: 43})\n",
      "Counter({0: 1370, 1: 130})\n",
      "Counter({0: 456, 1: 44})\n",
      "Fold 3 - Train: Counter({0: 1370, 1: 130}), Test: Counter({0: 456, 1: 44})\n",
      "Counter({0: 1370, 1: 130})\n",
      "Counter({0: 456, 1: 44})\n",
      "Fold 4 - Train: Counter({0: 1370, 1: 130}), Test: Counter({0: 456, 1: 44})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "fold_data = []  # List to store data from each fold\n",
    "\n",
    "seed = 42  # Use your desired random seed\n",
    "splitter = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "images = images / 255\n",
    "    # mean normalize\n",
    "images = (images - images.mean(dim=(0, 2, 3), keepdim=True)) / images.std(dim=(0, 2, 3), keepdim=True)\n",
    "for fold, (train_idx, test_idx) in enumerate(splitter.split(images, labels)):\n",
    "    X_train, X_test = images[train_idx], images[test_idx]\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "    # Count occurrences of each class in train and test sets\n",
    "    train_counter = Counter(y_train.numpy())\n",
    "    test_counter = Counter(y_test.numpy())\n",
    "    print(train_counter)\n",
    "    print(test_counter)\n",
    "    print(f\"Fold {fold + 1} - Train: {train_counter}, Test: {test_counter}\")\n",
    "\n",
    "    fold_data.append({\n",
    "        'fold': fold + 1,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'train_counter': train_counter,\n",
    "        'test_counter': test_counter\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=TensorDataset(X_train, y_train)\n",
    "test_dataset=TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "trainloader datatype:  torch.uint8\n",
      "testloader datatype:  torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh.mondal/miniconda3/envs/torch_space/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Loss: 0.00064813: 100%|██████████| 100/100 [52:00<00:00, 31.20s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9600\n",
      "Precision: 0.7674\n",
      "Recall: 0.7674\n",
      "F1 Score: 0.7674\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "trainloader datatype:  torch.uint8\n",
      "testloader datatype:  torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.00006198: 100%|██████████| 100/100 [47:26<00:00, 28.47s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9580\n",
      "Precision: 0.7292\n",
      "Recall: 0.8140\n",
      "F1 Score: 0.7692\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "trainloader datatype:  torch.uint8\n",
      "testloader datatype:  torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.00364517: 100%|██████████| 100/100 [04:07<00:00,  2.47s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9560\n",
      "Precision: 0.7750\n",
      "Recall: 0.7045\n",
      "F1 Score: 0.7381\n",
      "\n",
      "\n",
      "Fold:  4\n",
      "trainloader datatype:  torch.uint8\n",
      "testloader datatype:  torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.00020934: 100%|██████████| 100/100 [04:50<00:00,  2.91s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9600\n",
      "Precision: 0.9286\n",
      "Recall: 0.5909\n",
      "F1 Score: 0.7222\n",
      "\n",
      "\n",
      "Mean Accuracy:  tensor(0.9585, device='cuda:0')\n",
      "Mean Precision:  tensor(0.8000, device='cuda:0')\n",
      "Mean Recall:  tensor(0.7192, device='cuda:0')\n",
      "Mean F1:  tensor(0.7492, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=512\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "for fold_info in fold_data:\n",
    "    fold = fold_info['fold']\n",
    "    print(\"Fold: \", fold)\n",
    "    X_train = fold_info['X_train']\n",
    "    y_train = fold_info['y_train']\n",
    "    X_test = fold_info['X_test']\n",
    "    y_test = fold_info['y_test']\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoader for training and testing\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # Create and train the model\n",
    "    #print datatype of trainloader\n",
    "    print(\"trainloader datatype: \", train_loader.dataset.tensors[1].dtype)\n",
    "    print(\"testloader datatype: \", test_loader.dataset.tensors[0].dtype)\n",
    "    train_model = EfficientNetClassifier    (\n",
    "        models.efficientnet_b0, EfficientNet_B0_Weights, n_classes=2, activation=nn.ReLU(), dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    iter_losses, epoch_losses = train_fn(\n",
    "        train_model,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        dataloader=train_loader,\n",
    "        lr=3e-4,\n",
    "        epochs=100,\n",
    "        verbose=True,\n",
    "        wandb_log=False,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    with torch.no_grad():\n",
    "        pred_classes = train_model.predict_class(\n",
    "            dataloader=test_loader, batch_size=batch_size, verbose=True\n",
    "        ).to(device)\n",
    "\n",
    "    test_labels = y_test.to(device)\n",
    "    # Calculate and print metrics for each fold\n",
    "    \n",
    "    accuracy = accuracy_score(pred_classes,test_labels)\n",
    "    precision = precision_score( pred_classes,test_labels)\n",
    "    recall = recall_score( pred_classes,test_labels)\n",
    "    f1 = f1_score( pred_classes,test_labels)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    # Calculate and store metrics for each fold\n",
    "    accuracy_list.append(accuracy_score( pred_classes,test_labels))\n",
    "    precision_list.append(precision_score( pred_classes,test_labels))\n",
    "    recall_list.append(recall_score( pred_classes,test_labels))\n",
    "    f1_list.append(f1_score( pred_classes,test_labels))\n",
    "\n",
    "# Calculate and print the mean of metrics across all folds\n",
    "print(\"Mean Accuracy: \", sum(accuracy_list) / len(accuracy_list))\n",
    "print(\"Mean Precision: \", sum(precision_list) / len(precision_list))\n",
    "print(\"Mean Recall: \", sum(recall_list) / len(recall_list))\n",
    "print(\"Mean F1: \", sum(f1_list) / len(f1_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved tensors\n",
    "loaded_data = torch.load(\"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/tensor_data/test_data.pt\")\n",
    "\n",
    "# Access the tensors\n",
    "index1 = loaded_data['index']\n",
    "images1 = loaded_data['images']\n",
    "labels1= loaded_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2000, 3, 224, 224]), torch.Size([2000]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images1=images1[:2000]\n",
    "labels1=labels1[:2000]\n",
    "images1.shape, labels1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1308, 1: 192})\n",
      "Counter({0: 437, 1: 63})\n",
      "Fold 1 - Train: Counter({0: 1308, 1: 192}), Test: Counter({0: 437, 1: 63})\n",
      "Counter({0: 1309, 1: 191})\n",
      "Counter({0: 436, 1: 64})\n",
      "Fold 2 - Train: Counter({0: 1309, 1: 191}), Test: Counter({0: 436, 1: 64})\n",
      "Counter({0: 1309, 1: 191})\n",
      "Counter({0: 436, 1: 64})\n",
      "Fold 3 - Train: Counter({0: 1309, 1: 191}), Test: Counter({0: 436, 1: 64})\n",
      "Counter({0: 1309, 1: 191})\n",
      "Counter({0: 436, 1: 64})\n",
      "Fold 4 - Train: Counter({0: 1309, 1: 191}), Test: Counter({0: 436, 1: 64})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "fold_data1 = []  # List to store data from each fold\n",
    "\n",
    "seed = 42  # Use your desired random seed\n",
    "splitter = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "images1 = images1 / 255\n",
    "    # mean normalize\n",
    "images1 = (images1 - images1.mean(dim=(0, 2, 3), keepdim=True)) / images1.std(dim=(0, 2, 3), keepdim=True)\n",
    "for fold, (train_idx, test_idx) in enumerate(splitter.split(images1, labels1)):\n",
    "    X_train, X_test = images1[train_idx], images1[test_idx]\n",
    "    y_train, y_test = labels1[train_idx], labels1[test_idx]\n",
    "\n",
    "    # Count occurrences of each class in train and test sets\n",
    "    train_counter = Counter(y_train.numpy())\n",
    "    test_counter = Counter(y_test.numpy())\n",
    "    print(train_counter)\n",
    "    print(test_counter)\n",
    "    print(f\"Fold {fold + 1} - Train: {train_counter}, Test: {test_counter}\")\n",
    "\n",
    "    fold_data1.append({\n",
    "        'fold': fold + 1,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'train_counter': train_counter,\n",
    "        'test_counter': test_counter\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "trainloader datatype:  torch.uint8\n",
      "testloader datatype:  torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh.mondal/miniconda3/envs/torch_space/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Loss: 0.00078289: 100%|██████████| 100/100 [04:19<00:00,  2.59s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9540\n",
      "Precision: 0.8571\n",
      "Recall: 0.7619\n",
      "F1 Score: 0.8067\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "trainloader datatype:  torch.uint8\n",
      "testloader datatype:  torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.00210621: 100%|██████████| 100/100 [04:01<00:00,  2.42s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9540\n",
      "Precision: 0.8254\n",
      "Recall: 0.8125\n",
      "F1 Score: 0.8189\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "trainloader datatype:  torch.uint8\n",
      "testloader datatype:  torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.00147033: 100%|██████████| 100/100 [03:12<00:00,  1.92s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9460\n",
      "Precision: 0.8246\n",
      "Recall: 0.7344\n",
      "F1 Score: 0.7769\n",
      "\n",
      "\n",
      "Fold:  4\n",
      "trainloader datatype:  torch.uint8\n",
      "testloader datatype:  torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.00628338: 100%|██████████| 100/100 [03:52<00:00,  2.32s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9380\n",
      "Precision: 0.8113\n",
      "Recall: 0.6719\n",
      "F1 Score: 0.7350\n",
      "\n",
      "\n",
      "Mean Accuracy:  tensor(0.9480, device='cuda:0')\n",
      "Mean Precision:  tensor(0.8296, device='cuda:0')\n",
      "Mean Recall:  tensor(0.7452, device='cuda:0')\n",
      "Mean F1:  tensor(0.7844, device='cuda:0')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=512\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "for fold_info in fold_data1:\n",
    "    fold = fold_info['fold']\n",
    "    print(\"Fold: \", fold)\n",
    "    X_train = fold_info['X_train']\n",
    "    y_train = fold_info['y_train']\n",
    "    X_test = fold_info['X_test']\n",
    "    y_test = fold_info['y_test']\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoader for training and testing\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # Create and train the model\n",
    "    #print datatype of trainloader\n",
    "    print(\"trainloader datatype: \", train_loader.dataset.tensors[1].dtype)\n",
    "    print(\"testloader datatype: \", test_loader.dataset.tensors[0].dtype)\n",
    "    train_model = EfficientNetClassifier    (\n",
    "        models.efficientnet_b0, EfficientNet_B0_Weights, n_classes=2, activation=nn.ReLU(), dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    iter_losses, epoch_losses = train_fn(\n",
    "        train_model,\n",
    "        nn.CrossEntropyLoss(),\n",
    "        dataloader=train_loader,\n",
    "        lr=3e-4,\n",
    "        epochs=100,\n",
    "        verbose=True,\n",
    "        wandb_log=False,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    with torch.no_grad():\n",
    "        pred_classes = train_model.predict_class(\n",
    "            dataloader=test_loader, batch_size=batch_size, verbose=True\n",
    "        ).to(device)\n",
    "\n",
    "    test_labels = y_test.to(device)\n",
    "    # Calculate and print metrics for each fold\n",
    "    \n",
    "    accuracy = accuracy_score(pred_classes,test_labels)\n",
    "    precision = precision_score( pred_classes,test_labels)\n",
    "    recall = recall_score( pred_classes,test_labels)\n",
    "    f1 = f1_score( pred_classes,test_labels)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    # Calculate and store metrics for each fold\n",
    "    accuracy_list.append(accuracy_score( pred_classes,test_labels))\n",
    "    precision_list.append(precision_score( pred_classes,test_labels))\n",
    "    recall_list.append(recall_score( pred_classes,test_labels))\n",
    "    f1_list.append(f1_score( pred_classes,test_labels))\n",
    "\n",
    "# Calculate and print the mean of metrics across all folds\n",
    "print(\"Mean Accuracy: \", sum(accuracy_list) / len(accuracy_list))\n",
    "print(\"Mean Precision: \", sum(precision_list) / len(precision_list))\n",
    "print(\"Mean Recall: \", sum(recall_list) / len(recall_list))\n",
    "print(\"Mean F1: \", sum(f1_list) / len(f1_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 3, 224, 224]) torch.Size([2000])\n",
      "torch.Size([2000, 3, 224, 224]) torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "print(images.shape, labels.shape)\n",
    "print(images1.shape, labels1.shape)  \n",
    "images = images / 255\n",
    "    # mean normalize\n",
    "images = (images - images.mean(dim=(0, 2, 3), keepdim=True)) / images.std(dim=(0, 2, 3), keepdim=True)\n",
    "images1 = images1 / 255\n",
    "    # mean normalize\n",
    "images1 = (images1 - images1.mean(dim=(0, 2, 3), keepdim=True)) / images1.std(dim=(0, 2, 3), keepdim=True)\n",
    "train_dataset = TensorDataset(images, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=8)\n",
    "test_dataset = TensorDataset(images1, labels1)\n",
    "test_loaderr = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh.mondal/miniconda3/envs/torch_space/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Loss: 0.00442287: 100%|██████████| 100/100 [11:10<00:00,  6.71s/it]\n"
     ]
    }
   ],
   "source": [
    "train_model = EfficientNetClassifier(\n",
    "    models.efficientnet_b0,EfficientNet_B0_Weights, n_classes=2, activation=nn.ReLU(), dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "iter_losses, epoch_losses = train_fn(\n",
    "    train_model,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    dataloader=train_loader,\n",
    "    lr=3e-4,\n",
    "    epochs=100,\n",
    "    verbose=True,\n",
    "    wandb_log=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbc432c5cd0>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "Accuracy:  tensor(0.8715, device='cuda:0')\n",
      "Precision:  tensor(0.4953, device='cuda:0')\n",
      "Recall:  tensor(0.4118, device='cuda:0')\n",
      "F1:  tensor(0.4497, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(labels1))\n",
    "print(len(images1))\n",
    "test_dataset = TensorDataset(images1, labels1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=8)\n",
    "with torch.no_grad():\n",
    "    pred_classes =train_model.predict_class(\n",
    "        dataloader=test_loader, batch_size=254, verbose=True\n",
    "    ).to(device)\n",
    "print(len(pred_classes))\n",
    "test_labels = labels1.to(device)\n",
    "print(\"Accuracy: \", accuracy_score(pred_classes,test_labels))\n",
    "print(\"Precision: \", precision_score(pred_classes,test_labels))\n",
    "print(\"Recall: \", recall_score(pred_classes,test_labels))\n",
    "print(\"F1: \", f1_score(pred_classes,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_space",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
