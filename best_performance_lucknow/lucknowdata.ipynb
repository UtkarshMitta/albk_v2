{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from os.path import expanduser, join, basename, dirname\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from shutil import copy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from albk.data.utils import idx_to_locate\n",
    "use_disjoint_files = False\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from glob import glob\n",
    "from os.path import expanduser, join, basename, dirname\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from glob import glob\n",
    "from os.path import expanduser, join, basename, dirname\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from astra.torch.models import EfficientNetClassifier,EfficientNet_B0_Weights\n",
    "from astra.torch.utils import train_fn\n",
    "\n",
    "import torchvision.models as models\n",
    "from astra.torch.metrics import accuracy_score, f1_score, precision_score, recall_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_path = expanduser(\"/home/rishabh.mondal/bkdb/statewise/West_Bengal/\")\n",
    "images_path = expanduser('/home/rishabh.mondal/bkdb/districtwise/jhajhar/')\n",
    "zarr_files = [f for f in os.listdir(images_path) if f.endswith(\".zarr\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1713\n",
      "['/home/rishabh.mondal/bkdb/districtwise/jhajhar/28.60,76.59.zarr', '/home/rishabh.mondal/bkdb/districtwise/jhajhar/28.59,76.74.zarr', '/home/rishabh.mondal/bkdb/districtwise/jhajhar/28.45,76.64.zarr', '/home/rishabh.mondal/bkdb/districtwise/jhajhar/28.42,76.65.zarr', '/home/rishabh.mondal/bkdb/districtwise/jhajhar/28.60,76.46.zarr']\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for zarr_file in zarr_files:\n",
    "    image_path = join(images_path, zarr_file)\n",
    "    files.append(image_path)\n",
    "\n",
    "print(len(files))\n",
    "print(files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "\n",
    "def get_index_and_image(images_path):\n",
    "    images = []\n",
    "    index = []\n",
    "\n",
    "    # for zarr_file in zarr_files:\n",
    "    zarr_files = [f for f in os.listdir(images_path) if f.endswith(\".zarr\")]\n",
    "    for zarr_file in zarr_files:\n",
    "        image_path = join(images_path, zarr_file)\n",
    "        # print(image_path)\n",
    "        files.append(image_path)\n",
    "        image_ds = xr.open_zarr(image_path, consolidated=False)\n",
    "        for lat_lag, lon_lag in product(range(-2, 3), repeat=2):\n",
    "            images.append(torch.tensor(image_ds.sel(lat_lag=lat_lag, lon_lag=lon_lag)['data'].values)[np.newaxis, ...])\n",
    "            index.append((zarr_file, lat_lag, lon_lag))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_index_and_image(images_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 224, 3])\n"
     ]
    }
   ],
   "source": [
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42825, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "concatenated_images = torch.cat(images)\n",
    "concatenated_images= concatenated_images.permute(0,3,2,1)\n",
    "print(concatenated_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save_path=\"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/tensor_data/west_bengal.pt\"\n",
    "# torch.save({\n",
    "#     'images': concatenated_images\n",
    "# }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42825, 3, 224, 224])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images1=concatenated_images\n",
    "images1 = images1 / 255\n",
    "    # mean normalize\n",
    "images1 = (images1 - images1.mean(dim=(0, 2, 3), keepdim=True)) / images1.std(dim=(0, 2, 3), keepdim=True)\n",
    "images1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.zeros(len(images1))\n",
    "\n",
    "testset=TensorDataset(images1,labels)\n",
    "testloader=DataLoader(testset,batch_size=512,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = EfficientNetClassifier(\n",
    "    models.efficientnet_b0,None, n_classes=2, activation=nn.ReLU(), dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.load_state_dict(torch.load(\"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/Saved_model/delhi_whole_data_pretrain_aug.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [11:12<00:00,  8.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero predictions: 42278\n",
      "Number of one predictions: 547\n",
      "[26, 27, 38, 41, 43, 44, 80, 82, 328, 378, 500, 515, 650, 651, 672, 687, 896, 914, 1125, 1188, 1199, 1266, 1497, 1775, 1797, 1798, 2349, 2442, 2443, 2507, 2701, 2703, 2708, 2720, 2761, 3194, 3199, 3228, 3245, 3304, 3321, 3324, 3444, 3564, 3627, 3632, 3639, 4076, 4077, 4085, 4092, 4093, 4098, 4187, 4584, 4587, 4591, 4685, 4695, 5283, 5291, 5346, 5614, 5617, 5618, 5624, 5625, 5630, 5637, 5638, 5643, 5962, 5963, 5964, 5968, 5969, 6868, 6893, 6898, 7151, 7152, 7240, 7246, 7247, 7290, 7291, 7293, 7295, 7318, 7375, 7380, 7450, 7457, 7474, 7507, 7603, 7701, 7709, 7712, 7713, 7717, 7722, 7833, 7846, 7990, 7995, 8020, 8055, 8067, 8068, 8074, 8141, 8994, 9036, 9037, 9040, 9194, 9248, 9259, 9393, 9769, 9823, 9824, 9930, 9939, 9948, 9949, 10131, 10204, 10593, 11019, 11024, 11285, 11288, 11290, 11495, 11785, 11790, 11792, 11794, 11799, 11819, 11858, 11861, 11865, 11868, 12030, 12127, 12130, 12131, 12132, 12135, 12142, 12143, 12163, 12262, 12265, 12266, 12267, 12271, 12272, 12273, 12404, 12424, 12435, 12446, 12587, 12588, 12594, 12703, 12707, 13102, 13331, 13378, 13514, 13520, 13544, 13598, 13599, 13632, 13925, 14254, 14273, 14331, 14467, 14472, 14550, 14557, 14589, 14733, 14738, 15030, 15033, 15039, 15049, 15087, 15088, 15095, 15450, 15676, 15805, 15810, 15817, 15820, 15903, 15921, 15978, 15979, 15983, 16300, 16301, 16305, 16909, 16914, 16916, 17028, 17120, 17277, 17280, 17301, 17306, 17307, 17308, 17403, 17532, 17704, 17721, 17788, 17789, 17790, 17791, 17798, 17998, 17999, 18095, 18276, 18394, 18398, 18399, 18410, 18413, 18418, 18419, 18421, 18500, 18501, 18504, 18505, 18506, 18511, 18520, 18628, 18633, 18634, 18638, 18639, 18861, 18863, 18866, 18869, 19026, 19031, 19092, 19093, 19097, 19242, 19562, 19744, 20029, 20085, 20094, 20096, 20097, 20175, 20176, 20181, 20195, 20200, 20272, 20348, 20398, 20411, 20414, 20416, 20418, 20419, 20424, 20434, 20676, 20802, 20804, 20811, 20816, 21128, 21144, 21149, 21162, 21415, 21422, 21467, 21601, 21602, 21603, 21604, 21607, 21608, 21738, 21743, 21764, 22047, 22048, 22169, 22477, 22485, 22490, 22491, 22494, 22495, 22496, 22499, 22502, 22518, 22558, 22775, 22777, 22778, 22779, 22780, 22784, 22788, 22876, 22881, 22895, 22962, 22970, 23305, 23587, 23916, 23922, 23923, 23924, 24086, 24087, 24098, 24099, 24554, 24568, 24569, 24635, 24850, 24851, 24858, 25001, 25002, 25013, 25014, 25019, 25302, 25652, 25657, 25663, 25664, 25665, 25670, 25671, 25674, 25701, 25709, 26035, 26053, 26871, 26930, 27101, 27102, 27103, 27106, 27107, 27108, 27124, 27484, 27576, 28009, 28260, 28263, 28264, 28269, 28279, 28590, 28860, 28867, 28870, 28998, 29097, 29107, 29133, 29134, 29228, 29361, 29453, 29454, 29459, 29760, 29761, 29762, 29983, 29984, 29991, 30377, 30388, 30389, 30633, 30935, 30938, 30940, 30945, 30946, 30955, 31003, 31005, 31032, 31149, 31340, 31341, 31653, 31791, 31830, 31835, 31854, 31901, 31947, 31995, 32100, 32528, 32730, 32789, 33024, 33153, 33653, 34027, 34028, 34029, 34031, 34034, 34036, 34037, 34038, 34042, 34043, 34398, 34425, 34579, 34605, 34963, 34964, 35010, 35279, 35554, 35557, 35558, 35559, 35562, 35563, 35568, 35570, 36129, 36285, 36298, 36300, 36317, 36319, 36320, 36325, 36328, 36331, 36333, 36334, 36339, 36410, 36411, 36591, 36891, 36985, 36991, 37310, 37315, 37324, 37339, 37343, 37344, 37347, 37577, 37683, 37817, 37822, 37899, 37930, 38121, 38404, 38561, 38614, 38655, 39061, 39165, 39167, 39172, 39188, 39238, 39239, 39244, 39249, 39780, 39786, 39789, 39794, 39921, 39941, 40031, 40252, 40253, 40257, 40353, 40434, 40605, 40610, 40612, 40678, 40682, 40684, 40685, 40887, 41329, 41568, 41569, 41623, 41891, 42196, 42256, 42257, 42260, 42265, 42267, 42270, 42475, 42482, 42659, 42660, 42665]\n",
      "547\n"
     ]
    }
   ],
   "source": [
    "# train_model='/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/Saved_model/delhi_whole_data_train_model.pt'\n",
    "positive_image_indices = []\n",
    "save_dir = '/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/predicted_positive/jhajhar/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_classes =train_model.predict_class(\n",
    "        dataloader=testloader, batch_size=512, verbose=True\n",
    "    ).to(device)\n",
    "\n",
    "zero_predictions = torch.sum(pred_classes == 0)\n",
    "one_predictions = torch.sum(pred_classes == 1)\n",
    "\n",
    "print(\"Number of zero predictions:\", zero_predictions.item())\n",
    "print(\"Number of one predictions:\", one_predictions.item())\n",
    "images=images1\n",
    "for i in range(len(images)):\n",
    "    # Check if the prediction is positive (class 1)\n",
    "    if pred_classes[i] == 1:\n",
    "        normalized_image = (images[i] - images[i].min()) / (images[i].max() - images[i].min())\n",
    "        pil_image = transforms.ToPILImage()(normalized_image.cpu())\n",
    "\n",
    "        # append the index of the positive image\n",
    "        positive_image_indices.append(i)\n",
    "        \n",
    "        image_name = f\"positive_image_{i}.jpg\"\n",
    "        image_path = os.path.join(save_dir, image_name)\n",
    "        pil_image.save(image_path)            \n",
    "\n",
    "# Print the list of positive image indices\n",
    "print(positive_image_indices)\n",
    "print(len(positive_image_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_space",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
