{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from os.path import expanduser, join, basename, dirname\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from shutil import copy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from albk.data.utils import idx_to_locate\n",
    "use_disjoint_files = False\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from glob import glob\n",
    "from os.path import expanduser, join, basename, dirname\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from glob import glob\n",
    "from os.path import expanduser, join, basename, dirname\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from astra.torch.models import EfficientNetClassifier,EfficientNet_B0_Weights\n",
    "from astra.torch.utils import train_fn\n",
    "\n",
    "import torchvision.models as models\n",
    "from astra.torch.metrics import accuracy_score, f1_score, precision_score, recall_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_path = expanduser(\"/home/rishabh.mondal/bkdb/statewise/West_Bengal/\")\n",
    "images_path = expanduser('/home/rishabh.mondal/bkdb/districtwise/birbhum/')\n",
    "zarr_files = [f for f in os.listdir(images_path) if f.endswith(\".zarr\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4031\n",
      "['/home/rishabh.mondal/bkdb/districtwise/birbhum/24.23,87.97.zarr', '/home/rishabh.mondal/bkdb/districtwise/birbhum/23.84,87.83.zarr', '/home/rishabh.mondal/bkdb/districtwise/birbhum/23.94,87.50.zarr', '/home/rishabh.mondal/bkdb/districtwise/birbhum/23.81,87.44.zarr', '/home/rishabh.mondal/bkdb/districtwise/birbhum/23.68,87.57.zarr']\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for zarr_file in zarr_files:\n",
    "    image_path = join(images_path, zarr_file)\n",
    "    files.append(image_path)\n",
    "\n",
    "print(len(files))\n",
    "print(files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "\n",
    "def get_index_and_image(images_path):\n",
    "    images = []\n",
    "    index = []\n",
    "\n",
    "    # for zarr_file in zarr_files:\n",
    "    zarr_files = [f for f in os.listdir(images_path) if f.endswith(\".zarr\")]\n",
    "    for zarr_file in zarr_files:\n",
    "        image_path = join(images_path, zarr_file)\n",
    "        # print(image_path)\n",
    "        files.append(image_path)\n",
    "        image_ds = xr.open_zarr(image_path, consolidated=False)\n",
    "        for lat_lag, lon_lag in product(range(-2, 3), repeat=2):\n",
    "            images.append(torch.tensor(image_ds.sel(lat_lag=lat_lag, lon_lag=lon_lag)['data'].values)[np.newaxis, ...])\n",
    "            index.append((zarr_file, lat_lag, lon_lag))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_index_and_image(images_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 224, 3])\n"
     ]
    }
   ],
   "source": [
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100775, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "concatenated_images = torch.cat(images)\n",
    "concatenated_images= concatenated_images.permute(0,3,2,1)\n",
    "print(concatenated_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save_path=\"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/tensor_data/west_bengal.pt\"\n",
    "# torch.save({\n",
    "#     'images': concatenated_images\n",
    "# }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100775, 3, 224, 224])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images1=concatenated_images\n",
    "images1 = images1 / 255\n",
    "    # mean normalize\n",
    "images1 = (images1 - images1.mean(dim=(0, 2, 3), keepdim=True)) / images1.std(dim=(0, 2, 3), keepdim=True)\n",
    "images1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.zeros(len(images1))\n",
    "\n",
    "testset=TensorDataset(images1,labels)\n",
    "testloader=DataLoader(testset,batch_size=512,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='/home/vannsh.jani/brick_kilns/githubrepo/ML/model_50_no_ssl_features_imagenet.pth'\n",
    "\n",
    "path2='/home/vannsh.jani/brick_kilns/githubrepo/ML/model_50_no_ssl_classifier_imagenet.pth'\n",
    "model = torchvision.models.efficientnet_b0(pretrained=False)\n",
    "model.classifier = nn.Linear(1280,2)\n",
    "model.features.load_state_dict(torch.load(path))\n",
    "model.classifier.load_state_dict(torch.load(path2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model = EfficientNetClassifier(\n",
    "#     models.efficientnet_b0,None, n_classes=2, activation=nn.ReLU(), dropout=0.1\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_model.load_state_dict(torch.load(\"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/Saved_model/delhi_whole_data_pretrain_aug.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EfficientNet' object has no attribute 'predict_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     pred_classes \u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_class\u001b[49m(\n\u001b[1;32m      8\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtestloader, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m zero_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(pred_classes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m one_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(pred_classes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_space/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EfficientNet' object has no attribute 'predict_class'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# train_model='/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/Saved_model/delhi_whole_data_train_model.pt'\n",
    "positive_image_indices = []\n",
    "save_dir = '/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/predicted_positive/jhajhar/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # # with torch.no_grad():\n",
    "# # #     pred_classes =model.predict_class(\n",
    "# # #         dataloader=testloader, batch_size=512, verbose=True\n",
    "# # #     ).to(device)\n",
    "\n",
    "# # pred_classes = torch.zeros(len(images1))\n",
    "# # for i, (inputs, labels) in enumerate(testloader):\n",
    "# #     inputs, labels = inputs.to(device), labels.to(device)\n",
    "# #     outputs = model(inputs)\n",
    "# #     _, predicted = torch.max(outputs, 1)\n",
    "# #     pred_classes[i*512:(i+1)*512] = predicted\n",
    "# # zero_predictions = torch.sum(pred_classes == 0)\n",
    "# one_predictions = torch.sum(pred_classes == 1)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred_classes = torch.zeros(len(images1))\n",
    "    for i\n",
    "\n",
    "print(\"Number of zero predictions:\", zero_predictions.item())\n",
    "print(\"Number of one predictions:\", one_predictions.item())\n",
    "images=images1\n",
    "for i in range(len(images)):\n",
    "    # Check if the prediction is positive (class 1)\n",
    "    if pred_classes[i] == 1:\n",
    "        normalized_image = (images[i] - images[i].min()) / (images[i].max() - images[i].min())\n",
    "        pil_image = transforms.ToPILImage()(normalized_image.cpu())\n",
    "\n",
    "        # append the index of the positive image\n",
    "        positive_image_indices.append(i)\n",
    "        \n",
    "        image_name = f\"positive_image_{i}.jpg\"\n",
    "        image_path = os.path.join(save_dir, image_name)\n",
    "        pil_image.save(image_path)            \n",
    "\n",
    "# Print the list of positive image indices\n",
    "print(positive_image_indices)\n",
    "print(len(positive_image_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_space",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
